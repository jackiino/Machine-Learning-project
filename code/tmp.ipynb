{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ace3562-c00a-4980-89c1-8f4e03477846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446dcba2",
   "metadata": {},
   "source": [
    "# First Dataset: \"games_details.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a19c27",
   "metadata": {},
   "source": [
    "Have a look to the Datasets, one at a time. Firstly, \"games_details.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb63dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "games_details = pd.read_csv('../dataset/games_details.csv', low_memory=False)\n",
    "games_details.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f748cb7f-5afe-47f5-b7ac-27b5d698eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(games_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5e5e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio_missing_values prints the percentage of missing values in the column\n",
    "key1 = 'COMMENT'\n",
    "ratio_missing_values_column(games_details, key1)\n",
    "\n",
    "key2 = 'START_POSITION'\n",
    "ratio_missing_values_column(games_details, key2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38983976",
   "metadata": {},
   "source": [
    "'COMMENT': Since the number of valid values is very little wrt the size of the Dataset and there are not clear solutions to fill empty cells, we drop it.\n",
    "\n",
    "'START_POSITION': same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138e57b5",
   "metadata": {},
   "source": [
    "Moreover, we drop all the columns we think could lead to information leakage and also all columns that we think are not useful in the training model, such as the Nickname of the player. Our cleaned Dataset is the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a306e3c4-96e2-4f65-ac9e-c2167296134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Weird_rows = games_details['GAME_ID'] == 10500109\n",
    "games_details = games_details[~Weird_rows]\n",
    "\n",
    "columns_to_drop = ['MIN','COMMENT', 'PLAYER_NAME', 'NICKNAME', 'START_POSITION', 'COMMENT', 'TEAM_CITY', 'TEAM_ABBREVIATION', 'FGA', 'FG_PCT', 'FGM', 'FG3A', 'FG3_PCT', 'FTM', 'FTA', 'FT_PCT', 'OREB', 'DREB', 'REB', 'AST', 'STL', 'BLK', 'TO', 'PF', 'PTS', 'PLUS_MINUS']\n",
    "games_details = games_details.drop(columns = columns_to_drop) # se non conta, si toglie. Altrimenti\n",
    "# modificarla cambia il risultato\n",
    "games_details = games_details.dropna()\n",
    "games_details = games_details.reset_index(drop=True)\n",
    "\n",
    "games_details.info()\n",
    "games_details.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533ec61a",
   "metadata": {},
   "source": [
    "# Second Dataset: \"games.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875dbdc1",
   "metadata": {},
   "source": [
    "Now, pass to the second Dataset, which is \"games.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c968f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "games = pd.read_csv('../dataset/games.csv')\n",
    "games.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc14269-c091-4396-b5a6-ea5cdba2427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in games['GAME_DATE_EST']:\n",
    "    games['GAME_DATE_EST'] = games['GAME_DATE_EST'].replace(j,StringToDate(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9db9b5-0afd-45d6-8ca2-431cda1f5528",
   "metadata": {},
   "outputs": [],
   "source": [
    "games = games.sort_values(by='GAME_DATE_EST',ascending=False)\n",
    "games = games.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c6753c-43de-4190-9924-c22a030681e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = pd.read_csv('../dataset/teams.csv')\n",
    "teams.info()\n",
    "teams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e7ac04",
   "metadata": {},
   "source": [
    "# Third Dataset: \"ranking.csv\"\n",
    "Aggiungere solo ultima partita della squadra avversaria e di quella del giocatore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26993b3-bb79-46c0-8e5e-1979bcf95fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking = pd.read_csv('../dataset/ranking.csv')\n",
    "ranking.info()\n",
    "ranking = ranking.drop(columns=['LEAGUE_ID', 'RETURNTOPLAY']) # se soli zeri/NaN: rimosse\n",
    "ranking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f392bc-2ed6-43e7-b0a7-18f59410df31",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = teams.drop(columns=['LEAGUE_ID', 'MAX_YEAR', 'MIN_YEAR','ABBREVIATION','NICKNAME','YEARFOUNDED','CITY','ARENA','ARENACAPACITY','OWNER','GENERALMANAGER','HEADCOACH','DLEAGUEAFFILIATION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a2dc5c-203f-4377-a3a9-7b8408419567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes 2D arrays for past games and winrates for each team\n",
    "\n",
    "gamesTeamPlayed = [] #2D array of game_id's\n",
    "gamesTeamDates = [] #2D array of distances from last game (when defined)\n",
    "gamesTeamWinrates = [] #2D array of winrates in the past 3 matches (when defined)\n",
    "for i in teams['TEAM_ID']:\n",
    "    gamesHome = games['HOME_TEAM_ID'] == i\n",
    "    gamesVisitor = games['VISITOR_TEAM_ID'] == i\n",
    "    gamesTeamI = [x or y for x, y in zip(gamesHome,gamesVisitor)]\n",
    "    tempGamesTeamPlayed = []\n",
    "    for l in range(len(gamesTeamI)):\n",
    "        if gamesTeamI[l]:\n",
    "            tempGamesTeamPlayed.append(games['GAME_ID'][l])\n",
    "    gamesTeamPlayed.append(tempGamesTeamPlayed)\n",
    "    indices = []\n",
    "    counter = 0\n",
    "    for j in gamesTeamI:\n",
    "        if j:\n",
    "            indices.append(counter)\n",
    "        counter = counter + 1\n",
    "    gamesTeamDates.append([games['GAME_DATE_EST'][j] for j in indices])\n",
    "    winrates = []\n",
    "    for k in range(len(indices)-1,-1,-1):\n",
    "        if len(indices)-1-k == 0:\n",
    "            winrates.append(-1)\n",
    "        elif len(indices)-1-k == 1:\n",
    "            home = 1 if (games['HOME_TEAM_ID'][indices[k+1]] == i) else 0\n",
    "            winrate = home*(games['HOME_TEAM_WINS'][indices[k+1]])+(1-home)*(1-games['HOME_TEAM_WINS'][indices[k+1]])\n",
    "            winrates.append(winrate)\n",
    "        elif len(indices)-1-k == 2:\n",
    "            home = 1 if (games['HOME_TEAM_ID'][indices[k+1]] == i) else 0\n",
    "            winrate = (home*(games['HOME_TEAM_WINS'][indices[k+1]])+(1-home)*(1-games['HOME_TEAM_WINS'][indices[k+1]]) + winrates[1])/2\n",
    "            winrates.append(winrate)\n",
    "        elif len(indices)-1-k >= 3:\n",
    "            home = 1 if (games['HOME_TEAM_ID'][indices[k+1]] == i) else 0\n",
    "            winrate = home*(games['HOME_TEAM_WINS'][indices[k+1]])+(1-home)*(1-games['HOME_TEAM_WINS'][indices[k+1]])\n",
    "            home = 1 if (games['HOME_TEAM_ID'][indices[k+2]] == i) else 0\n",
    "            winrate = winrate + home*(games['HOME_TEAM_WINS'][indices[k+2]])+(1-home)*(1-games['HOME_TEAM_WINS'][indices[k+2]])\n",
    "            home = 1 if (games['HOME_TEAM_ID'][indices[k+3]] == i) else 0\n",
    "            winrate = winrate + home*(games['HOME_TEAM_WINS'][indices[k+3]])+(1-home)*(1-games['HOME_TEAM_WINS'][indices[k+3]])\n",
    "            winrates.append(winrate/3)\n",
    "    winratesRev = []\n",
    "    for h in range(len(winrates)-1,-1,-1):\n",
    "        winratesRev.append(winrates[h])\n",
    "    gamesTeamWinrates.append(winratesRev)\n",
    "    \n",
    "\n",
    "        \n",
    "def DiffOppWin(game_id,team_id):\n",
    "    indGameId = games.index[games['GAME_ID'] == game_id].tolist()[0]\n",
    "    indTeamId = teams['TEAM_ID'].index[teams['TEAM_ID'] == team_id].tolist()[0]\n",
    "    date = games['GAME_DATE_EST'][indGameId]\n",
    "    dateId = gamesTeamDates[indTeamId].index(date)\n",
    "    diff = -1\n",
    "    if dateId == len(gamesTeamDates[indTeamId])-1:\n",
    "        diff = -1\n",
    "    else:\n",
    "        diff = (date - gamesTeamDates[indTeamId][gamesTeamDates[indTeamId].index(date)+1])\n",
    "    opposing = 0\n",
    "    if games['HOME_TEAM_ID'][indGameId] == team_id:\n",
    "        opposing = games['VISITOR_TEAM_ID'][indGameId]\n",
    "    else:\n",
    "        opposing = games['HOME_TEAM_ID'][indGameId]\n",
    "    winrate = gamesTeamWinrates[indTeamId][gamesTeamPlayed[indTeamId].index(game_id)]\n",
    "    return [diff, opposing, winrate]\n",
    "\n",
    "def RefinedWinrate(game_id,home_id,visitor_id): #This is meant to edit games, not games_details\n",
    "    indGameId = games.index[games['GAME_ID'] == game_id].tolist()[0]\n",
    "    indTeamId = teams['TEAM_ID'].index[teams['TEAM_ID'] == home_id].tolist()[0]\n",
    "    indOppTeamId = teams['TEAM_ID'].index[teams['TEAM_ID'] == visitor_id].tolist()[0]\n",
    "    previousGames1 = [gamesTeamPlayed[indTeamId][j] for j in range(gamesTeamPlayed[indTeamId].index(indGameId),len(gamesTeamPlayed[indTeamId]))]\n",
    "    previousGames2 = [gamesTeamPlayed[indOppTeamId][j] for j in range(gamesTeamPlayed[indOppTeamId].index(indGameId),len(gamesTeamPlayed[indOppTeamId]))]\n",
    "    commonGames = [id for id in previousGames1 if id in previousGames2]\n",
    "    if len(commonGames) == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        wins = 0\n",
    "        for i in commonGames:\n",
    "            wins = wins + games['HOME_TEAM_WINS'][i]\n",
    "        wins = wins / len(commonGames)\n",
    "        return wins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090aba0f-7db7-4d19-a58c-b737fd2da9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds a column with distance from the last game, winrate and opponent team for every entry in games_details (circa ?? minuti sul fisso, 20 sul portatile)\n",
    "\n",
    "games_details['DATE_DIFF'] = ''\n",
    "games_details['OPPOSING_TEAM'] = ''\n",
    "games_details['WINRATE'] = ''\n",
    "for i in range(len(games_details['DATE_DIFF'])):\n",
    "    games_details.loc[i,'DATE_DIFF'] = DiffOppWin(games_details['GAME_ID'][i],games_details['TEAM_ID'][i])[0]\n",
    "    games_details.loc[i,'OPPOSING_TEAM'] = DiffOppWin(games_details['GAME_ID'][i],games_details['TEAM_ID'][i])[1]\n",
    "    games_details.loc[i,'WINRATE'] = DiffOppWin(games_details['GAME_ID'][i],games_details['TEAM_ID'][i])[2]\n",
    "\n",
    "# Aggiungere qui le triplozze medie (usando il fatto che averageThreePointers è ordinato come game_details rispetto ad ogni giocatore)\n",
    "# RICORDARSI DI TOGLIERE LE RIGHE CON -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e430d909-c275-4cc5-9f38-679f88196857",
   "metadata": {},
   "outputs": [],
   "source": [
    "games_details.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0683047-5902-4249-a25c-038bfd3f1636",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(games[games['GAME_STATUS_TEXT']=='Final'].equals(games)) # so no useful information, Final is the content of each cell\n",
    "games = games.drop(columns=['GAME_STATUS_TEXT'])\n",
    "\n",
    "games_details = complete_games_details(games_details, games)\n",
    "\n",
    "games_details['TEAM_ID'].astype(str)\n",
    "games_details['OPPOSING_TEAM_ID'].astype(str) # trasformare in stringhe nome così  da non lavorare\n",
    "# con numeri enormi vicini ma usare l'encoder e pace\n",
    "\n",
    "ratio_missing_values_df(games_details) # print percentage of rows in which there is at least one\n",
    "# missing value\n",
    "games_details = games_details.dropna() # drop all the rows with nans\n",
    "games_details = games_details.reset_index(drop=True) # adjust the indexing\n",
    "\n",
    "games_details.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac3cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "games_details.head(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4963fdd8-a1a4-4baa-b42c-4bcdb4307862",
   "metadata": {},
   "source": [
    "# *Nota*:\n",
    "Quando il dataset è incompleto, le funzioni che ho scritto non vanno correttamente perché è possibile che droppando le righe si perdano informazioni che permettono di collegare un dataset all'altro. Ci sono quindi molti missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ac674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.all(games_details['OPPOSING_TEAM'] == games_details['OPPOSING_TEAM_ID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d751986",
   "metadata": {},
   "outputs": [],
   "source": [
    "games_details = games_details.drop(columns=['OPPOSING_TEAM_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace6052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "games_details.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c22172",
   "metadata": {},
   "outputs": [],
   "source": [
    "games_details.to_csv('../dataset/dataset_completo.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35472166-9e23-406e-9d12-a6b01887762e",
   "metadata": {},
   "source": [
    "# Fourth Dataset: \"teams.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710c2fa1-a683-4915-9d1f-890c1c79f2b4",
   "metadata": {},
   "source": [
    "Looking the Dataset we can notice that some arena capacity values are missing. We've decided to fill it searching the values on google:\\\n",
    "-Smoothie King Center: 17,805 seats;\\\n",
    "-Barclays Center: 17.732 seats;\\\n",
    "-Wells Fargo Center: 20,318 seats;\\\n",
    "-Talking Stick Resort Arena: 17,071 seats;\n",
    "\n",
    "Moreover, Amway Center capcity seems to be wrong because its value is 0. So, we correct it:\\\n",
    "-Amway Center: 18,846 seats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3516da55-e238-46b9-9cb4-e86b6c2012f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "players = pd.read_csv('../dataset/players.csv')\n",
    "players.info()\n",
    "players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd4a08d-3c5a-4720-a32d-9c8179902191",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams.loc[2, 'ARENACAPACITY'] = 17805.0\n",
    "teams.loc[12, 'ARENACAPACITY'] = 17732.0\n",
    "teams.loc[14, 'ARENACAPACITY'] = 18846.0\n",
    "teams.loc[16, 'ARENACAPACITY'] = 20318.0\n",
    "teams.loc[17, 'ARENACAPACITY'] = 17071.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f286cefe-eef7-4e16-a59a-537529396f9d",
   "metadata": {},
   "source": [
    "# Fifth Dataset: 'players.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7863645",
   "metadata": {},
   "source": [
    "# Learning phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d563f29-affb-427e-9322-5e2b4dc1fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "# label encoder e non one-hot encoding per evitare di aumentare di molto\n",
    "# dimensionalità del dataset e perché le date e simili hanno effettivamente un ordine\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbd1098-d778-49d6-ab5b-c0124c603003",
   "metadata": {},
   "source": [
    "We will evaluate the performance of the regression algorithms via k-fold cross-validation.\n",
    "Before doing that, we choose hyperparameters by means of hyperparameter tuning (quelli su RF sono in realtà più o meno inutili perché si sa già la tendenza al variare di $n_{trees}$ e si ha già la $p$ ottimale. Ma già che c'eravamo...)\n",
    "\n",
    "To understand the importance of each variable during the decision process, Gini importance is not as effective as feature ablation. Yet the latter is not implemented in scikit-learn, therefore we use Gini as measurement of the importance of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de701016-523c-45d0-8efa-512ab97005a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "games_details = games_details.drop(columns=['GAME_ID'])\n",
    "games_details = games_details.sample(frac=0.05)\n",
    "\n",
    "categorical_columns = games_details.select_dtypes(include=['object']).columns\n",
    "label_encoder = LabelEncoder()\n",
    "games_details[list(categorical_columns)] = games_details[list(categorical_columns)].apply(label_encoder.fit_transform)\n",
    "\n",
    "games_details = games_details.dropna()\n",
    "\n",
    "X = games_details.loc[:, games_details.columns != 'FG3M']\n",
    "y = games_details['FG3M']\n",
    "\n",
    "reg_metrics = pd.DataFrame(columns=['Model', 'MAE', 'MSE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0a104a-b8c2-493f-b6b2-dfe459148389",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 8\n",
    "\n",
    "num_features = X.shape[1]\n",
    "p = int(np.ceil(num_features / 3))\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 500],\n",
    "    'max_features': ['sqrt', 'log2', p] \n",
    "}\n",
    "\n",
    "rf_regressor = RandomForestRegressor()\n",
    "scaler1 = StandardScaler()\n",
    "X_scaled = scaler1.fit_transform(X) # questo non sarebbe legittimo\n",
    "\n",
    "grid_search = GridSearchCV(rf_regressor, rf_param_grid, cv=num_folds, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "rf_best_params = grid_search.best_params_\n",
    "print(rf_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae2feb-8099-4457-be17-08187b7b6b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_kfolds = KFold(n_splits=num_folds, shuffle=True)\n",
    "gini_importances = pd.DataFrame(columns=X.columns)\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(rf_kfolds.split(X)):\n",
    "\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    rf_regressor = RandomForestRegressor(n_estimators = rf_best_params['n_estimators'], max_features = rf_best_params['max_features'])\n",
    "    \n",
    "    rf_regressor.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = rf_regressor.predict(X_test)\n",
    "    \n",
    "    current_rf_mae = mean_absolute_error(y_test, y_pred)\n",
    "    current_rf_mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    reg_metrics.loc[len(reg_metrics)] = ['RF', current_rf_mae, current_rf_mse]\n",
    "\n",
    "    importances = rf_regressor.feature_importances_\n",
    "    gini_importances.loc[fold] = importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbf6161-2adb-4b30-ab53-4a29075008da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_kfolds = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "for train_idx, test_idx in dummy_kfolds.split(X):\n",
    "    \n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    dummy_regressor = DummyRegressor()\n",
    "    dummy_regressor.fit(X_train, y_train)\n",
    "    \n",
    "    y_dummy_pred = dummy_regressor.predict(X_test)\n",
    "\n",
    "    current_dummy_mae = mean_absolute_error(y_test, y_dummy_pred)\n",
    "    current_dummy_mse = mean_squared_error(y_test, y_dummy_pred)\n",
    "\n",
    "    reg_metrics.loc[len(reg_metrics)] = ['DUMMY', current_dummy_mae, current_dummy_mse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddd7a0d-b759-49b9-aaa4-5ceb9f3cb8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_param_grid = {\n",
    "    'n_neighbors': [1, 5, 10],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "\n",
    "knn_regressor = KNeighborsRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(knn_regressor, knn_param_grid, cv=num_folds, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_scaled, y)\n",
    "\n",
    "knn_best_params = grid_search.best_params_\n",
    "print(knn_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea449770-d9a3-47fc-9954-5d82bb2b2fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_kfolds = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "for train_idx, test_idx in knn_kfolds.split(X):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    knn_regressor = KNeighborsRegressor(n_neighbors = knn_best_params['n_neighbors'], p = knn_best_params['p'])\n",
    "    knn_regressor.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = knn_regressor.predict(X_test)\n",
    "    \n",
    "    current_knn_mae = mean_absolute_error(y_test, y_pred)\n",
    "    current_knn_mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    reg_metrics.loc[len(reg_metrics)] = ['KNN', current_knn_mae, current_knn_mse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df84774-a0c3-4c04-9199-6537df679f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_param_grid = {\n",
    "    'C': [0.1, 1, 5],\n",
    "    'gamma': [0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "sv_regressor = SVR()\n",
    "\n",
    "grid_search_svm = GridSearchCV(sv_regressor, svm_param_grid, cv=num_folds, scoring='neg_mean_squared_error')\n",
    "grid_search_svm.fit(X_scaled, y)\n",
    "\n",
    "svm_best_params = grid_search_svm.best_params_\n",
    "print(svm_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e466bc16-f6b2-4219-b0a2-642f9153b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_kfolds = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "for train_idx, test_idx in sv_kfolds.split(X):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    sv_regressor = SVR(C = svm_best_params['C'], gamma = svm_best_params['gamma'])\n",
    "    sv_regressor.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = sv_regressor.predict(X_test)\n",
    "    \n",
    "    current_sv_mae = mean_absolute_error(y_test, y_pred)\n",
    "    current_sv_mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    reg_metrics.loc[len(reg_metrics)] = ['SVM', current_sv_mae, current_sv_mse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3854fe67-1213-4985-9dd7-e2d75aca58ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg_metrics)\n",
    "fig, axs = plt.subplots(1, 2, figsize= (12, 4))\n",
    "sns.boxplot(data=reg_metrics, x=\"MAE\", y=\"Model\", hue = 'Model', ax=axs[0], palette=sns.color_palette('Paired')[1::2])\n",
    "sns.boxplot(data=reg_metrics, x=\"MSE\", y=\"Model\", hue = 'Model', ax=axs[1], palette=sns.color_palette('Paired')[1::2])\n",
    "axs[1].set_yticklabels('')\n",
    "axs[1].set_ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4586af17-0c76-4a3b-afe3-ccd0254880d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gini_importances)\n",
    "melted_gini = gini_importances.melt(var_name='Column')\n",
    "\n",
    "# Create boxplot with seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='Column', y='value', data=melted_gini, hue='Column', palette='Set3')\n",
    "\n",
    "plt.title('Boxplot of Gini importances')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Values')\n",
    "plt.legend(title='Column')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984921d3-50d5-4016-a2d9-82b998579bac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
